# ğŸœ Itadaki ML - Recipe Calorie Prediction System

_"Itadaki" signifie "bon appÃ©tit" en japonais_

## ğŸ“ Projet de certification DEV IA - Alyra

Ce projet constitue le **projet final** de la certification **DÃ©veloppeur Intelligence Artificielle** d'Alyra. Il implÃ©mente un systÃ¨me de prÃ©diction du niveau calorique des recettes utilisant l'apprentissage automatique.

**CompÃ©tences Ã©valuÃ©es (Bloc 3 : Appliquer des techniques d'analyse IA via des algorithmes d'apprentissage automatique)** :

- **C1** : sÃ©lectionner l'algorithme d'apprentissage le plus adaptÃ© en comparant les performances et les caractÃ©ristiques des diffÃ©rentes familles d'algorithmes afin d'apporter une rÃ©ponse pertinente Ã  la problÃ©matique mÃ©tier rencontrÃ©e
- **C2** : prÃ©parer et transformer des donnÃ©es en utilisant des techniques de prÃ©traitement (preprocessing) pour les adapter aux spÃ©cificitÃ©s du modÃ¨le d'apprentissage automatique choisi
- **C3** : entraÃ®ner un modÃ¨le d'apprentissage automatique en optimisant une loss function (fonction de coÃ»t) Ã  partir des donnÃ©es d'entraÃ®nement afin de permettre Ã  l'algorithme d'effectuer le moins d'erreurs possibles selon des indicateurs de succÃ¨s clairement dÃ©finis

## ğŸ¯ Vue d'ensemble

SystÃ¨me d'intelligence artificielle qui **prÃ©dit les calories des recettes** Ã  partir de leurs ingrÃ©dients. Ce projet explore une **approche mÃ©thodique** d'amÃ©lioration itÃ©rative en testant diffÃ©rents algorithmes et optimisations de preprocessing.

## ğŸš€ Choix de l'approche : Machine Learning avant Deep Learning

### ğŸ¯ StratÃ©gie progressive validÃ©e

**Pourquoi commencer par ML traditionnel ?**

1. **ğŸ” ComprÃ©hension du problÃ¨me** : ML classique permet d'identifier les patterns nutritionnels
2. **âš¡ RapiditÃ© de dÃ©veloppement** : itÃ©rations rapides vs complexitÃ© DL
3. **ğŸª InterprÃ©tabilitÃ©** : features importantes comprÃ©hensibles par les nutritionnistes
4. **ğŸ“Š Baseline solide** : Ã©tablir un benchmark avant de complexifier
5. **ğŸ¯ EfficacitÃ© dÃ©montrÃ©e** : ML peut suffire pour ce type de problÃ¨me

**RÃ©sultat :** ML traditionnel s'avÃ¨re **satisfaisant** (53% accuracy sur 3 classes), validant cette approche qui rÃ©pond au besoin fonctionnel exprimÃ© par l'Ã©quipe.

## ğŸ“ˆ Ã‰volution progressive - 3 versions itÃ©ratives

Ce projet documente une **approche mÃ©thodique** avec 3 versions d'amÃ©lioration continue :

### ğŸ”¸ Version 1 (v1) : XGBoost + NLP Basique âœ…

**Approche :** PremiÃ¨re implÃ©mentation avec feature engineering classique

**ğŸ—ï¸ Architecture :**

```
IngrÃ©dients â†’ TF-IDF (500 features) + Features numÃ©riques (15) â†’ XGBoost
```

**ğŸ”§ Composants :**

- **TF-IDF** : vectorisation basique des ingrÃ©dients (ngram_range=(1,2))
- **Feature Engineering** : compteurs et ratios par catÃ©gorie nutritionnelle
- **XGBoost** : paramÃ¨tres par dÃ©faut avec optimisation lÃ©gÃ¨re

**ğŸ“Š Features numÃ©riques (15) :**

- **Compteurs** : nb_fat, nb_sugar, nb_protein, nb_vegetable, nb_grain, nb_drink, nb_spice
- **Ratios** : fat_ratio, sugar_ratio, protein_ratio, etc. (pour normaliser par nb d'ingrÃ©dients)
- **Total** : n_ingredients

**âš ï¸ DÃ©couverte clÃ© :** **TF-IDF seul ne suffit pas !** Les features numÃ©riques sont essentielles.

**ğŸ¯ RÃ©sultats v1 :**

- **Test accuracy :** ~50-51%
- **Apprentissage :** l'approche est viable mais perfectible

### ğŸ”¸ Version 2 (v2) : XGBoost + NLP optimisÃ© âœ…

**Approche :** optimisation du preprocessing NLP et normalisation des ingrÃ©dients

**ğŸ—ï¸ Architecture amÃ©liorÃ©e :**

```
IngrÃ©dients â†’ Normalisation â†’ TF-IDF (300 features, ngram=(1,1)) + Features numÃ©riques â†’ XGBoost optimisÃ©
```

**ğŸ”§ AmÃ©liorations majeures :**

1. **ğŸ¯ Normalisation intelligente des ingrÃ©dients :**

   ```python
   ingredient_normalization = {
       'brown_sugar': 'sugar',
       'white_sugar': 'sugar',
       'granulated_sugar': 'sugar',
       'extra_virgin_olive_oil': 'olive_oil',
       'unsalted_butter': 'butter',
       # ... +200 normalisations
   }
   ```

2. **ğŸ“ NGrams optimisÃ©s :**

   - **ngram_range=(1,1)** au lieu de (1,2)
   - **Logique :** avec underscores (`olive_oil`), les ingrÃ©dients sont dÃ©jÃ  des tokens uniques
   - **RÃ©sultat :** meilleur regroupement, moins de bruit

3. **ğŸª CatÃ©gories d'ingrÃ©dients Ã©tendues :**

   ```python
   fat_ingredients = {'butter', 'olive_oil', 'vegetable_oil', 'coconut_oil', ...}
   protein_ingredients = {'chicken', 'beef', 'fish', 'eggs', 'tofu', ...}
   sugar_ingredients = {'sugar', 'honey', 'syrup', 'brown_sugar', ...}
   # + vegetable, grain, spice, drink ingredients
   ```

4. **âš™ï¸ XGBoost optimisÃ© :**
   ```python
   XGBClassifier(
       n_estimators=200,
       max_depth=6,
       learning_rate=0.1,
       subsample=0.8,
       colsample_bytree=0.8,
       reg_alpha=0.2,
       reg_lambda=1.2
   )
   ```

**ğŸ¯ RÃ©sultats v2 :**

- **Test accuracy :** **53.49%** (+2-3% vs v1)
- **Train accuracy :** 61.09%
- **Gap d'overfitting :** 7.6% (bien maÃ®trisÃ©)
- **Performance par classe :**
  - BAS : 57% precision, 67% recall âœ…
  - MOYEN : 57% precision, 51% recall âœ…
  - HAUT : 46% precision, 42% recall (plus difficile)

**âœ… AmÃ©lioration confirmÃ©e :** le preprocessing intelligent amÃ©liore significativement les rÃ©sultats des tests (notamment sur le cas "moyen") !

### ğŸ”¸ Version 3 (v3) : Random Forest - test de robustesse âš¡

**Approche :** tester si un modÃ¨le plus "simple" et robuste peut Ã©galer XGBoost

**ğŸŒ² Pourquoi Random Forest ?**

- **RÃ©putation de robustesse** : Moins d'overfitting naturellement
- **class_weight='balanced'** : Gestion automatique des classes dÃ©sÃ©quilibrÃ©es
- **SimplicitÃ©** : Moins d'hyperparamÃ¨tres Ã  optimiser que XGBoost
- **ParallÃ©lisation** : Excellente sur CPU multi-core

**ğŸ—ï¸ Architecture v3 :**

```
IngrÃ©dients â†’ Normalisation (v2) â†’ TF-IDF (300 features, ngram=(1,1)) + Features numÃ©riques â†’ Random Forest
```

**ğŸ”§ Configuration Random Forest :**

```python
RandomForestClassifier(
    n_estimators=200,           # Nombre d'arbres
    max_depth=7,                # Profondeur limitÃ©e (anti-overfitting)
    min_samples_split=10,       # Min Ã©chantillons pour diviser
    min_samples_leaf=4,         # Min Ã©chantillons par feuille
    max_features=0.3,           # 30% des features par split
    bootstrap=True,             # Ã‰chantillonnage avec remise (force de Random Forest)
    criterion='entropy',        # CritÃ¨re de division (on a testÃ© gini Ã©galement mais entropy meilleur rÃ©sultat donc optimisation pour ne laisser que entropy)
    class_weight='balanced'     # Ã‰quilibrage automatique
)
```

**âš–ï¸ ParamÃ¨tres anti-overfitting :**

- **max_depth â‰¤ 7** : Ã©vite les arbres trop profonds
- **min_samples_leaf â‰¥ 4** : force la gÃ©nÃ©ralisation
- **max_features = 0.3** : alÃ©atoire contrÃ´lÃ© pour diversitÃ©

**ğŸ¯ RÃ©sultats v3 :**

- **Test accuracy :** 52.94% (lÃ©gÃ¨rement infÃ©rieur Ã  XGBoost v2)
- **Train accuracy :** 96.60% (âš ï¸ overfitting important si mal configurÃ©)
- **Avantage :** class_weight='balanced' amÃ©liore la classe HAUT
- **Temps :** plus long que XGBoost (~20-25 min vs 10 min et SHAP explainer trÃ¨s long)

## ğŸ“Š Comparaison des 3 versions

| Version | Algorithme    | NLP      | Test Accuracy | Train Accuracy | Gap   | Points forts           |
| ------- | ------------- | -------- | ------------- | -------------- | ----- | ---------------------- |
| **v1**  | XGBoost       | Basique  | ~50-51%       | ~58%           | ~7%   | Baseline rapide        |
| **v2**  | XGBoost       | OptimisÃ© | **53.30%** âœ… | 58.96%         | 5.66% | **Meilleur Ã©quilibre** |
| **v3**  | Random Forest | OptimisÃ© | 52.15%        | 62.08%         | 9.93% | Robustesse thÃ©orique   |

**ğŸ† Gagnant : Version 2 (XGBoost optimisÃ©)**

### ğŸ” Analyse dÃ©taillÃ©e XGBoost v2 vs Random Forest v3

**XGBoost v2 (meilleur) :**

- **Test accuracy :** 53.30%
- **Train accuracy :** 58.96%
- **Gap overfitting :** 5.66% (excellent contrÃ´le)
- **Performance par classe :**
  - BAS : 56% precision, 68% recall
  - MOYEN : 57% precision, 50% recall
  - HAUT : 46% precision, 42% recall

**Random Forest v3 :**

- **Test accuracy :** 52.15%
- **Train accuracy :** 62.08%
- **Gap overfitting :** 9.93% (surapprentissage plus important)
- **Performance par classe :**
  - BAS : 56% precision, 67% recall
  - MOYEN : 56% precision, 50% recall
  - HAUT : 44% precision, 40% recall

### âš ï¸ Test anti-overfitting Random Forest (Ã©chec)

**Configuration testÃ©e :**

```python
# ParamÃ¨tres trÃ¨s restrictifs pour Ã©viter l'overfitting
param_rf_restrictive = {
    'n_estimators': [50, 100, 200, 300, 500],
    'max_depth': [3, 5, 7],  # TrÃ¨s limitÃ©
    'min_samples_leaf': [4, 8],  # TrÃ¨s restrictif
    'min_samples_split': [20],  # TrÃ¨s restrictif
    'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],
    'criterion': ['entropy']
}
```

**RÃ©sultat :** **0.49 d'accuracy** (chute drastique de 3% vs configuration Ã©quilibrÃ©e)

**Pourquoi l'Ã©chec :**

- **max_depth â‰¤ 7** : trop restrictif, modÃ¨le sous-apprend
- **min_samples_leaf â‰¥ 4** : force la gÃ©nÃ©ralisation mais perd en prÃ©cision
- **min_samples_split = 20** : arbres trop simples pour capturer les patterns nutritionnels

**LeÃ§on apprise :** L'Ã©quilibre entre contrÃ´le de l'overfitting et capacitÃ© d'apprentissage est crucial. Des paramÃ¨tres trop restrictifs peuvent dÃ©grader significativement les performances.

## ğŸ¯ Architecture finale optimale (v2)

### ğŸ“ Pipeline de preprocessing

```python
# 1. Nettoyage et normalisation
ingredients â†’ clean_text() â†’ sort_ingredients() â†’ normalize_ingredients()

# 2. Vectorisation TF-IDF
tfidf = TfidfVectorizer(
    max_features=300,
    min_df=100,
    max_df=0.95,
    ngram_range=(1, 1),
    stop_words=None
)

# 3. Features numÃ©riques (15)
[n_ingredients, nb_fat, nb_sugar, nb_protein, nb_vegetable, nb_grain, nb_drink, nb_spice,
 fat_ratio, sugar_ratio, protein_ratio, vegetable_ratio, grain_ratio, drink_ratio, spice_ratio]

# 4. Combinaison
X_combined = hstack([tfidf_features, numeric_features])
```

### ğŸª ModÃ¨le final

```python
XGBClassifier(
    objective='multi:softprob',
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.1,
    reg_alpha=0.2,
    reg_lambda=1.2,
    random_state=42
)
```

## ğŸ”¬ Insights techniques majeurs

### ğŸ¯ DÃ©couvertes de preprocessing

1. **Normalisation critique :** 200+ mappings d'ingrÃ©dients similaires amÃ©liorent lÃ©gÃ¨rement l'accuracy et donnent de meilleurs rÃ©sultats de tests
2. **NGrams (1,1) optimal :** avec underscores, les bigrammes ajoutent plus de bruit que de signal
3. **Features numÃ©riques essentielles :** TF-IDF seul = ~40%, TF-IDF + numeric = 53%

### ğŸ“Š Features les plus importantes

**Top 5 TF-IDF (ingrÃ©dients) :**

1. **butter** - indicateur de richesse calorique
2. **sugar** - sucrant direct
3. **olive_oil** - matiÃ¨re grasse saine
4. **garlic** - aromate de base vs sophistiquÃ©
5. **lettuce** - indicateur de lÃ©gÃ¨retÃ©

**Top 5 NumÃ©riques :**

1. **fat_ratio** - proportion de matiÃ¨res grasses
2. **sugar_ratio** - proportion de sucrants
3. **n_ingredients** - complexitÃ© de la recette
4. **protein_ratio** - proportion de protÃ©ines
5. **vegetable_ratio** - proportion de lÃ©gumes

## ğŸš€ Perspectives d'amÃ©lioration

### ğŸ”® Version 4 (future) : Feature Engineering avancÃ©

**Nouvelle feature catÃ©gorielle envisagÃ©e (Ã  calculer grÃ¢ce aux features numÃ©riques):**

```python
recipe_type = [
    'DESSERT',
    'SNACK',
    'APPETIZER',
    'MAIN_DISH',
    'DRINK',
]
```

**Impact attendu :** +2-5% accuracy par contextualisation culinaire

### ğŸ¯ Autres amÃ©liorations possibles

1. **Instructions NLP** : utiliser les instructions qui contiennent les quantitÃ©s, mÃ©thodes de cuisson etc mais cela nÃ©cessite un traitement NLP complexe
2. **Dataset plus consÃ©quent** : utiliser un dataset plus riche permettrait d'obtenir certainement de meilleurs rÃ©sultats (le ML a besoin d'une quantitÃ© importante de donnÃ©es)

## ğŸ“¥ Dataset

**Food Ingredients Dataset**

- **228,430 recettes uniques** avec ingrÃ©dients et instructions
- **Classification calorique** : BAS (<215 cal), MOYEN (215-430 cal), HAUT (>430 cal)
- **IngrÃ©dients dÃ©taillÃ©s** en format texte
- **Distribution Ã©quilibrÃ©e** des classes (33% chacune)

**ğŸ”— Source :** [Food Ingredients and Recipe Dataset with Raw Text](https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-raw-text) sur Kaggle

**ğŸ“ Fichier utilisÃ© :** `RAW_recipes.csv`

## ğŸ“š Structure du projet

```
itadaki_ML/
â”œâ”€â”€ ğŸ““ NOTEBOOKS
â”‚   â”œâ”€â”€ calorie_prediction_notebook_xgboost_v1.ipynb    # v1 - Baseline XGBoost
â”‚   â”œâ”€â”€ calorie_prediction_notebook_xgboost_v2.ipynb    # v2 - XGBoost optimisÃ© âœ…
â”‚   â””â”€â”€ calorie_prediction_notebook_random_forest_v3.ipynb # v3 - Random Forest
â”œâ”€â”€ ğŸ“ DONNÃ‰ES
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ RAW_recipes.csv                             # Dataset original
â”œâ”€â”€ ğŸ“ MODÃˆLES
â”‚   â””â”€â”€ models/                                         # ModÃ¨les sauvegardÃ©s
â””â”€â”€ ğŸ“ REPORTS
    â”œâ”€â”€ top_1000_ingredients.txt                        # Analyse frÃ©quence ingrÃ©dients
    â””â”€â”€ diffÃ©rents notebooks exportÃ©s
```

## ğŸ¯ RÃ©sultats finaux et mÃ©triques

### ğŸ† Performance optimale (v2)

**MÃ©triques globales :**

- **Test accuracy :** **53.30%** (vs 33% alÃ©atoire = +60% d'amÃ©lioration)
- **Train accuracy :** 58.96%
- **Gap d'overfitting :** 5.66% (excellent contrÃ´le)

**Performance par classe :**

```
              precision    recall  f1-score   support
         bas       0.56      0.68      0.61     15208  âœ… TrÃ¨s bonne
       moyen       0.57      0.50      0.53     15032  âœ… Satisfaisante
        haut       0.46      0.42      0.44     15446  âš–ï¸ Acceptable

    accuracy                           0.53     45686
   macro avg       0.53      0.53      0.53     45686
weighted avg       0.53      0.53      0.53     45686
```

### ğŸ¯ Contexte d'Ã©valuation

**Pourquoi 53% est trÃ¨s bon au vu de l'algorithme et des donnÃ©es :**

1. **ğŸ¥„ QuantitÃ©s manquantes** : "flour, sugar, butter" â†’ pancake ou gÃ¢teau au chocolat ?
2. **ğŸ”¥ MÃ©thodes de cuisson** : grillÃ© vs frit change drastiquement les calories
3. **ğŸ“ Portions variables** : mÃªme recette, tailles diffÃ©rentes
4. **ğŸŒ VariabilitÃ© culturelle** : "rice" peut Ãªtre 100-400 calories selon prÃ©paration

## ğŸ“š Technologies utilisÃ©es

- **Scikit-learn** : TF-IDF, preprocessing, mÃ©triques
- **XGBoost** : algorithme principal (meilleurs rÃ©sultats)
- **Random Forest** : alternative testÃ©e pour robustesse
- **Pandas/NumPy** : manipulation de donnÃ©es
- **Matplotlib/Seaborn** : visualisations et analyses
- **SHAP** : interprÃ©tabilitÃ© des modÃ¨les
- **Jupyter** : environnement de dÃ©veloppement

## ğŸ¯ Conclusions

### **ğŸ† Apprentissages majeurs**

1. **ML traditionnel suffisant** : pas besoin de DL pour ce problÃ¨me (car le besoin Ã©tait minimaliste) mais on irait bien plus loin avec du DL
2. **Preprocessing critique** : la normalisation des ingrÃ©dients apporte +3% de performance
3. **Features hybrides** : TF-IDF + features numÃ©riques = combinaison gagnante
4. **XGBoost optimal** : surpasse Random Forest sur ce type de donnÃ©es sparses
5. **53% = benchmark trÃ¨s correct** : performance remarquable pour la prÃ©diction nutritionnelle (avec 2 classes "pas trop calorique" | "calorique", on obtiendrait certainement une trÃ¨s bonne accuracy)

### **ğŸš€ Recommandations**

1. **DÃ©ployer v2** : XGBoost v2 ready peut Ãªtre dÃ©ployÃ© (cela ne pourra que s'amÃ©liorer et cela permet de crÃ©er la partie MLOPS)
2. **Enrichir dataset** : quantitÃ©s et mÃ©thodes de cuisson
3. **Applications mÃ©tier** : apps nutrition, recommandations alimentaires

---

**Projet rÃ©alisÃ© dans le cadre de la certification DÃ©veloppeur IA - Alyra**

_L'approche progressive v1 â†’ v2 â†’ v3 dÃ©montre l'importance du preprocessing intelligent et valide XGBoost comme choix optimal pour la classification nutritionnelle._

**ğŸ† BENCHMARK Ã‰TABLI : 53.30% d'accuracy en classification calorique avec XGBoost optimisÃ©**

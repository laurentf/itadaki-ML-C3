Seuils statiques pour bas/medium/haut => mieux de calculer avec 33%/33%/33%

Utilisation one hot encoding sur suite de mot => explosion mémoire (préférer vecteurs tf-idf ou word bags)

Trop d'itérations au départ dans le RandomizedSearch; c'est scientifique => après 20 essais aléatoires => ~95% de l'optimum possible.
2/3h vs 10/15min...

Oubli du LabelEncoder pour les classes et ordre non conservé si on utilise pas ça

Apres LabelEncoder les labels deviennent des numpy arrays

Il a aussi fallu décoder dans la prédiction et ajouter target_names dans classification_report

RandomizedSearchCV serait bien plus rapide pour 80% des perf mais je veux le max possible donc à voir :)



